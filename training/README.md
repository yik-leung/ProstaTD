# Surgical Triplet Detection

A simple training script for surgical action triplet detection using YOLO.

## ğŸš€ Installation

```bash
# Clone the repository

# yolo series 
cd training/framework/lib
# in python3.8.20, pytorch2.4.1, cuda12.4 
pip install -e ./
cd ..

# detr series (for def-detr, PARIP, MCIT-IG.....)
cd detection/Deformable-DETR
# python3.8.20, pytorch2.4.1, cuda12.4 
pip install -r requirements.txt
cd ./models/ops
sh ./make.sh
cd ..

```

If you want to use our calculation metrics, please follow this [ivtdmetrics](../ivtdmetrics/).

## ğŸ“‚ YOLO Dataset Structure (5-Fold Cross Validation)

The dataset should be organized with 5-fold cross validation structure:

```
dataset_yolo_triplet/
â”œâ”€â”€ triplet_maps_v2.txt         # Triplet to tool mapping file
â”œâ”€â”€ split1/
â”‚   â”œâ”€â”€ dataset_triplet.yaml    # Dataset configuration file for split1
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ images/             # Training images (.jpg)
â”‚   â”‚   â””â”€â”€ labels/             # Training labels (.txt)
â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â”œâ”€â”€ images/             # Validation images (.jpg)
â”‚   â”‚   â””â”€â”€ labels/             # Validation labels (.txt)
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ images/             # Test images (.jpg)
â”‚       â””â”€â”€ labels/             # Test labels (.txt)
â”œâ”€â”€ split2/
â”‚   â”œâ”€â”€ dataset_triplet.yaml    # Dataset configuration file for split2
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ split3/
â”‚   â”œâ”€â”€ dataset_triplet.yaml
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ split4/
â”‚   â”œâ”€â”€ dataset_triplet.yaml
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â””â”€â”€ split5/
    â”œâ”€â”€ dataset_triplet.yaml
    â”œâ”€â”€ train/
    â”œâ”€â”€ val/
    â””â”€â”€ test/
```

## ğŸ“ COCO Dataset Structure (5-Fold Cross Validation)

The COCO format dataset should be organized as follows:

```
dataset_coco_triplet/
â”œâ”€â”€ triplet_maps_v2.txt         # Triplet to tool mapping file
â”œâ”€â”€ split1/
â”‚   â”œâ”€â”€ train/                  # Training images (.jpg)
â”‚   â”œâ”€â”€ val/                    # Validation images (.jpg)
â”‚   â”œâ”€â”€ test/                   # Test images (.jpg)
â”‚   â”œâ”€â”€ train_annotations.json  # Training annotations in COCO format
â”‚   â”œâ”€â”€ val_annotations.json    # Validation annotations in COCO format
â”‚   â””â”€â”€ test_annotations.json   # Test annotations in COCO format
â”œâ”€â”€ split2/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ train_annotations.json
â”‚   â”œâ”€â”€ val_annotations.json
â”‚   â””â”€â”€ test_annotations.json
â”œâ”€â”€ split3/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ train_annotations.json
â”‚   â”œâ”€â”€ val_annotations.json
â”‚   â””â”€â”€ test_annotations.json
â”œâ”€â”€ split4/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ train_annotations.json
â”‚   â”œâ”€â”€ val_annotations.json
â”‚   â””â”€â”€ test_annotations.json
â””â”€â”€ split5/
    â”œâ”€â”€ train/
    â”œâ”€â”€ val/
    â”œâ”€â”€ test/
    â”œâ”€â”€ train_annotations.json
    â”œâ”€â”€ val_annotations.json
    â””â”€â”€ test_annotations.json
```

### ğŸ·ï¸ Label Format

#### YOLO Format
Each label file contains annotations in YOLO format:

```
class_id center_x center_y width height
```

Where:
- `class_id`: Triplet class ID (0-88)
- `center_x, center_y`: Normalized center coordinates (0-1)
- `width, height`: Normalized bounding box dimensions (0-1)

#### COCO Format
Annotations are stored in JSON files following COCO format:

```json
{
    "images": [
        {
            "id": 1,
            "file_name": "1.jpg",
            "width": 1920,
            "height": 1080
        }
    ],
    "annotations": [
        {
            "id": 1,
            "image_id": 1,
            "category_id": 1,
            "bbox": ["x", "y", "width", "height"],
            "area": 12345,
            "iscrowd": 0
        }
    ],
    "categories": [
        {
            "id": 1,
            "name": "instrument_verb_target",
            "supercategory": "triplet"
        }
    ]
}
```

Where:
- `bbox`: [x, y, width, height] in absolute pixel coordinates 
- `category_id`: Triplet class ID (1-89, COCO format starts from 1)
- `area`: Bounding box area in pixels

<a id="bench_yolo"></a>
### ğŸ‹ï¸ Yolo Training Example

```bash
python train_yolo.py \
    --data /path/to/dataset.yaml \
    --model yolo12m.pt \
    --epochs 150 \
    --batch 16 \
    --mapping-file /path/to/triplet_maps_v2.txt \
    --apply-ivt-metrics true
```

### ğŸ§ª Yolo Testing Example

```bash
python train_yolo.py \
    --data /path/to/dataset.yaml \
    --test-only \
    --model yolo12m.pt \
    --name yolov12 \
    --weights /path/to/best.pt \
    --mapping-file /path/to/triplet_maps_v2.txt
```

### âš™ï¸ Yolo Parameters

- **`--agnostic-nms`**: Use class-agnostic NMS (default: False)
- **`--tool-nms`**: Apply tool-based NMS patch
- **`--mapping-file`**: Path to tripletâ†’tool mapping file
- **`--apply-ivt-metrics`**: Apply IVT Detection metrics patch for mAP calculation (default: True)

<a id="bench_detr"></a>
### ğŸ¤– DETR Training and Testing Examples

```bash
bash detr/train_triplet_detr.sh    # training
```

```bash
bash detr/run_triplet_inference.sh # testing
```

```bash
python framework/train_rtdetr.py # training and testing
```

Make sure to modify the paths in the script according to your setup.


All these results above can calculate the mAP using following command: 
```bash
python calculate_ivtd.py # based on ivtdmetrics
```

<a id="postprocess"></a>
### ğŸ–¼ï¸ Postprocess (Visualization in Triplet-labelme or SurgLabel)

To visualize the original image shape of predictions in our custom annotation tool using LabelMe format, please follow the steps below.

```bash
# Navigate to the postprocess directory:
cd postprocess

# Convert predictions to LabelMe format:
python convert_yolo_to_labelme.py

# Add transformation parameters to the converted LabelMe JSON:
# This script applies the shape size parameters (e.g., 640x640) from the conversion step to the prediction results in LabelMe format.
python add_transform_info_yolo_predict_json.py

# Restore predictions to the original image shape:
python resize_640.py --restore

# Open our triplet-labelme or SurgLabel to visualize the annotation
```

## ğŸ™ Acknowledgments

This code is based on the [Ultralytics YOLO](https://github.com/ultralytics/ultralytics), [Deformable DETR](https://github.com/fundamentalvision/Deformable-DETR) and [RT-DETR](https://github.com/lyuwenyu/RT-DETR).
